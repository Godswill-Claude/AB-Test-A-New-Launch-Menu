{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9PRhjQAtH+SaeybWofm4T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Godswill-Claude/AB-Test-A-New-Launch-Menu/blob/main/Copy_of_Natural_Language_Processing_(NLP)_Natural_Language_Toolkit_(NLTK)_Demo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP is a branch of artificial intelligence (AI) concerned with giving computers the ability to understand text and spoken words much the same way that humans can.\n",
        "\n",
        "NLP tools and libraries are found in the NLTK. Python provides a wide range of tools and libraries for NLP tasks.\n",
        "\n",
        "NLTK is a set of open source python modules used to work with human language and data for applying statistical NLP. NLTK requires python 3.0 and above to run."
      ],
      "metadata": {
        "id": "o60FAJ17QcB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## THE NLP PROCESS WORKFLOW\n",
        "\n",
        "Best practices suggests we take the following steps, in the stated order, when performing any NLP operation\n",
        "1. Tokenization\n",
        "2. Stopwords removal\n",
        "3. Stemming and lemmatization\n",
        "4. POS tagging\n",
        "5. Information retrieval"
      ],
      "metadata": {
        "id": "tMhOXDL-bWYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETTING UP THE NLTK ENVIRONMENT"
      ],
      "metadata": {
        "id": "2GkmNcFdReWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_k-GUhYR0Vh",
        "outputId": "48af5b6d-6da9-4f8c-a220-56addd9599f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EI8RRKNQVCW"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmCBvatdUSvN",
        "outputId": "146ba618-c550-4d93-b3e0-1dcd1dd161ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To test that nltk is successfully installed and imported in our machine\n",
        "\n",
        "from nltk.corpus import brown\n",
        "brown.words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVTcFM7IS6sD",
        "outputId": "5fb598c6-90a0-4930-d33a-919a1330bf1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SOME NLTK TOOLS FOR TEXT EXTRACTION AND PRE-PROCESSING"
      ],
      "metadata": {
        "id": "B2uUOWzMUja1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tokenization\n",
        "\n",
        "This is the process of removing sensitive data and placing unique symbols of identification in its place to return essential information.\n",
        "It involves breaking texts into words and sentences.\n",
        "\n",
        "We have\n",
        "1. Sentence tokenization\n",
        "2. Word tokenization"
      ],
      "metadata": {
        "id": "EupTrEb4UrLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sentence tokenization demo"
      ],
      "metadata": {
        "id": "ZnbaR-MgV9Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "_7idD1A4WHFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The sent_tokenize function from the nltk library requires the punkt resource to be installed\n",
        "# in order to tokenize the text. To install the punkt resource using the nltk.download function:\n",
        "\n",
        "!python3 -m nltk.downloader punkt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbnRZSi9SNZa",
        "outputId": "894f54ba-8070-4bd9-88fc-f69ca681d772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will assign a string of text to \"text\" and tokenize the string\n",
        "\n",
        "text = \"This is some random text we are using to demonstrate the implementation of text tokenization. Please follow through\""
      ],
      "metadata": {
        "id": "pENMhDRZU2lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling sent_tokenize function on \"text\"\n",
        "\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2ruaG1JQ1To",
        "outputId": "968d16ea-94f0-492f-f182-27a4eaf62d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This is some random text we are using to demonstrate the implementation of text tokenization.', 'Please follow through']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that our \"text\" string has been splitted into two parts or sentences, with a comma in-between"
      ],
      "metadata": {
        "id": "yNotuwFNUx6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word tokenization demo"
      ],
      "metadata": {
        "id": "-7YhK-0hS53Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "MBKOvBP3TD4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling the word_tokenize function on \"text\"\n",
        "\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI30wITZVRM_",
        "outputId": "ce19e1d8-49a0-4330-9ff1-00095e412670"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'some', 'random', 'text', 'we', 'are', 'using', 'to', 'demonstrate', 'the', 'implementation', 'of', 'text', 'tokenization', '.', 'Please', 'follow', 'through']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that our \"text\" string has been splitted into words, with a comma in-between each"
      ],
      "metadata": {
        "id": "78rJ-x0rVw-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. N-grams\n",
        "\n",
        "In language, the meaning of sentences is dependent on the order of the words.\n",
        "\n",
        "N-gram is a simple language model that assigns probabilities to sequences of words and sentences.\n",
        "\n",
        "In technical terms, they can be defined as neighbouring sequences of items in a document.\n",
        "\n",
        "N-grams is dependent on the number of splits you want to perform. 1-gram splits the sentence into each word. 2-gram splits the sentence into groups of two words, 3-gram splits into groups of three words, and so on.\n",
        "\n",
        "N-grams help retain the context of a word because when a word is standing alone, it may be lost in translation"
      ],
      "metadata": {
        "id": "Ku48bQRiWPhn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Stop words\n",
        "\n",
        "These are natual language words which have very little meaning, e.g. or, at, they, etc, and other prepositions.\n",
        "\n",
        "These words take up space in the database and increase the processing time.\n",
        "\n",
        "The nltk data library has a list of stop words that are stored in over 16 languages"
      ],
      "metadata": {
        "id": "QINz74vSYulY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to install the stopwords corpus using the NLTK Downloader.\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bjYJr2Eaa-c",
        "outputId": "9d1c4348-aee9-4595-c1ab-8f1fd510743f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Nrcessary import statements for stopwords task\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "JNBzSOVJcZvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # The following command sets the stopwords to English Language and shows the list of stop\n",
        "# words that are found in the nltk.corpus library\n",
        "set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c059ZIXIblwa",
        "outputId": "b3bcec89-62c2-46f5-94d2-3e64fdb3f12c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can simply use the stopwords function to remove the above list of stopwords from our text while performing NLP"
      ],
      "metadata": {
        "id": "EZLL7W9WddiK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Stemming\n",
        "\n",
        "Involves reducing a text/stream of data/words to stem or base form by removing suffixes.e,g, reducing the words \"dancing\" or \"danced\" to the base form \"dance\", reducing \"cooking\" or \"cooked\" to \"cook\", etc.\n",
        "\n",
        "Stemming algorithms include Porter stemmer, Lancaster stemmer, Snowball stemmer.\n",
        "\n",
        "We will use the Porter stemmer to demonstrate stemming in python here"
      ],
      "metadata": {
        "id": "nsPNcgiheBVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to install the stopwords corpus using the NLTK Downloader.\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5M_h-QmBfaM",
        "outputId": "ca55764a-f1cb-4057-ff7f-4365a0c58eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary import statements for stemming task\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "id": "Y0EQildXeuv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # The following command sets the stopwords to English Language and shows the list of stop\n",
        "# words that are found in the nltk.corpus library\n",
        "set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCkt-9xUABvM",
        "outputId": "15477d47-8410-4582-e123-8c90fcb6dd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning PorterStemmer to a variable ps\n",
        "\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "wPVT56zLAgA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# But first, we have to tokenize our text before stemming.\n",
        "# We will continue to use the \"text\" created\n",
        "\n",
        "words = word_tokenize(text)"
      ],
      "metadata": {
        "id": "dnorwkjhB6o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, going into stemming of the words in \"text\", we create a for loop\n",
        "\n",
        "for w in words:\n",
        "  print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WJVspJ6Dsjv",
        "outputId": "c4dfa0d3-084b-4f5e-840a-b28095144bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thi\n",
            "is\n",
            "some\n",
            "random\n",
            "text\n",
            "we\n",
            "are\n",
            "use\n",
            "to\n",
            "demonstr\n",
            "the\n",
            "implement\n",
            "of\n",
            "text\n",
            "token\n",
            ".\n",
            "pleas\n",
            "follow\n",
            "through\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that some words in the output cell above have been reduced to their base form, even though some of the base forms are not accurate"
      ],
      "metadata": {
        "id": "Ss_T2aAKeBKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Lemmatization\n",
        "\n",
        "This is the process of grouping together the different inflected forms of a word so they can be analysed as a single item"
      ],
      "metadata": {
        "id": "5DrdV-3aFV7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required import statements\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iGCD4ETF5dL",
        "outputId": "4ddad52d-1f45-4cb7-9594-32c29ae44995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assigning WordNetLemmatizer to the variable lemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "bn6fmnReHuMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying lemmatizer to a string \"drinks\"\n",
        "\n",
        "print(\"drinks:\", lemmatizer.lemmatize(\"drinks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmSs9Y_eImOG",
        "outputId": "0b1eb548-aa22-42d8-b71d-80f991c86a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drinks: drink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that \"drinks\" has been lemmatized to \"drink\""
      ],
      "metadata": {
        "id": "LqmNsDuKLMsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying lemmatizer to another string \"corpora\"\n",
        "\n",
        "print(\"corpora:\", lemmatizer.lemmatize(\"corpora\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyeRwGRgKMJa",
        "outputId": "e7db6e6e-fc00-47d0-e7d0-83005b4fbca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpora: corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that \"corpora\" has been lemmatized to \"corpus\""
      ],
      "metadata": {
        "id": "sxfdlGZtLUud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying lemmatizer to another string \"Sunsets\"\n",
        "\n",
        "print(\"Sunsets:\", lemmatizer.lemmatize(\"Sunsets\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hei7boTvK_m1",
        "outputId": "c6c917ec-0835-433c-b5eb-8c11cb3a16a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sunsets: Sunsets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that \"Sunsets\" has been lemmatized to \"Sunsets\" or left unchanged"
      ],
      "metadata": {
        "id": "YBG2ImFCLx1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Part of speech (POS) tagging\n",
        "\n",
        "This is the process of marking words in a corpus (a collection of written texts or a body of writing on a particular subject) to a corresponding part of speech tag based on its context and definition.\n",
        "\n",
        "POS tags are useful in building name, entity recognition, in lemmatization, and in extracting relationships between words."
      ],
      "metadata": {
        "id": "ORH62KM3McFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required import statements\n",
        "\n",
        "from nltk.tag import DefaultTagger"
      ],
      "metadata": {
        "id": "8Miml4o8OOcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining our tag\n",
        "\n",
        "tagging = DefaultTagger(\"NN\")"
      ],
      "metadata": {
        "id": "39fk76VaOcTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, doing the actual tagging\n",
        "\n",
        "tagging.tag([\"hello\", \"world\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWK9MqtDPJsu",
        "outputId": "e5929d6d-b751-4651-9143-58c3548abfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('hello', 'NN'), ('world', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that each word in the list has been tagged NN"
      ],
      "metadata": {
        "id": "kqVgm0SrPhJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Name entity recognition (NER)\n",
        "\n",
        "This seeks to extract a real-world entity from the text and sort it into pre-defined categories such as people, locations, organizations, dates, etc.\n",
        "\n",
        "A typical NER output for a sample text could be as follows:\n",
        "\n",
        "--Person: Pendo Manjele, Manny Loore, Maria Parker, John, etc (that is, NER would get these names and tag them to Person)\n",
        "\n",
        "--Location: Lusaka, Kigali, Lagos, etc (that is, if it NER identifies the location, it will tag it as location and extract the information\n",
        "\n",
        "--Organization: AT&T, Chevron, etc (that is, if there is organization info in a stream of text NER would extract it out and tag it as such).\n",
        "\n",
        "NER is generally based on grammar rules and supervised models"
      ],
      "metadata": {
        "id": "e_NcH8gfP0Ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required import statements\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpsrgj1-SZPI",
        "outputId": "26341ce8-cf8e-45b8-b670-087d61cb5ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialising a body of text to be used as demo for NER\n",
        "\n",
        "text_body = \"Microsoft Corporation, headquartered in Redmond, Washington, announced a strategic partnership with Tesla, Inc. to develop autonomous vehicles.\""
      ],
      "metadata": {
        "id": "7TweczT1SncX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we perform word tokenization the text_body\n",
        "\n",
        "tokenized_text_body = nltk.word_tokenize(text_body)"
      ],
      "metadata": {
        "id": "B_i26T6GUL2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we tag our sentences using pos\n",
        "\n",
        "tag_sentences = nltk.pos_tag(tokenized_text_body)"
      ],
      "metadata": {
        "id": "jqQGCj-qVDdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we apply Chunking to our tagged sentences\n",
        "\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "ner_chunked_sentences = nltk.ne_chunk(tag_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAqUSyvAV5EB",
        "outputId": "b2894f30-bd13-43f8-9d6c-6ff1cb59f377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we create an empty list we will call \"ne\"\n",
        "ne = []\n",
        "\n",
        "# Creating a for loop to iterate through ner_chunked_sentences\n",
        "for tagged_tree in ner_chunked_sentences:\n",
        "  if hasattr(tagged_tree, 'label'):\n",
        "    entity_name = ''.join(c[0] for c in tagged_tree.leaves())\n",
        "    entity_type = tagged_tree.label()\n",
        "    ne.append((entity_name, entity_type))\n",
        "    print(ne)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRiXlAPgXCv2",
        "outputId": "b42a55a3-91ea-439d-927f-2d148682bf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Microsoft', 'PERSON')]\n",
            "[('Microsoft', 'PERSON'), ('Corporation', 'ORGANIZATION')]\n",
            "[('Microsoft', 'PERSON'), ('Corporation', 'ORGANIZATION'), ('Redmond', 'GPE')]\n",
            "[('Microsoft', 'PERSON'), ('Corporation', 'ORGANIZATION'), ('Redmond', 'GPE'), ('Washington', 'GPE')]\n",
            "[('Microsoft', 'PERSON'), ('Corporation', 'ORGANIZATION'), ('Redmond', 'GPE'), ('Washington', 'GPE'), ('Tesla', 'PERSON')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen above, all relevant parts of the body of text has been tagged appropriately. There could be a few mis-matches, though"
      ],
      "metadata": {
        "id": "7valzexqZalG"
      }
    }
  ]
}